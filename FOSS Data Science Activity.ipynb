{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6484d22",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Welcome to the Consumer Data Research Centre's workshop for the Festival of Social Science!\n",
    "\n",
    "By working through this notebook and following along with the accompanying video, you can learn more about the type of work a data scientist might do, and start writing your very own code.\n",
    "\n",
    "We’ll provide you with a large dataset of newspaper headlines from around the world, and you'll have the chance to discover examples of how the use of words differs between the United Kingdown and the United States. By following a typical data science project life-cycle, we’ll work through the process of gaining insights from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5472b-7c51-4fde-a7b4-05a71646eb18",
   "metadata": {},
   "source": [
    "# What does this process look like?\n",
    "\n",
    "\n",
    "When conducting data science we can broadly break this down into a few key areas;\n",
    "\n",
    " - Data Collection\n",
    " - Data Processing\n",
    " - Data Analysis\n",
    " - Data Visualisation and Dissemination\n",
    "\n",
    "We already have the data so we’ll be focussing on the other steps today.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b82f978",
   "metadata": {
    "tags": []
   },
   "source": [
    "# About This Notebook\n",
    "\n",
    "This interactive document you’re looking at is a called a Jupyter Notebook. Notebooks like this one are used a lot in universities and academia as a learning resource. \n",
    "\n",
    "Notebooks allow us to wrap up all the data, code and 'packages' (tools that can complete different tasks) in one place to make sharing our work easier so each person doesn’t have to individually download everything.\n",
    "\n",
    "#### Important notice: As this is a remote document, if you are inactive for more than 10 minutes it may disconnect from the server.\n",
    "#### If this happens, please close the page and re-enter through the link.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa110e2b-2e39-4edd-87e0-c29af967c947",
   "metadata": {},
   "source": [
    "# Running the Code\n",
    "\n",
    "All the code you need to use today is contained in the grey cells in this document. There are a few simple ways to make sure the code has worked:\n",
    "\n",
    " - To run a cell of code, you can select a cell by clicking on it, then click the “Run” button on the tool bar at the top of the page OR you can press the Ctrl and Enter keys together.\n",
    " - Once you have done this and the code has finished (this may take a few seconds depending on the task) the result will appear below the cell.\n",
    "\n",
    "Try running the code below to see what happens. \n",
    "\n",
    "##### Remember if you click into the cell you can either press the \"Run\" button in the toolbar above (the play button!) or press the Ctrl and Enter keys together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10026243",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World!\")\n",
    "print(17 + 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9177986-3699-4302-b1db-18c485993b75",
   "metadata": {},
   "source": [
    "You should see that it has printed the phrase “Hello World!” and calculated a sum at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18270f",
   "metadata": {},
   "source": [
    "# What is Python?\n",
    "\n",
    "\n",
    "Python is the name of the programming language we'll be using in this workshop. It is a very popular coding language used by researchers and businesses, and is an in-demand skill in the world of data science.\n",
    "\n",
    "Don't worry about understanding every piece of code you see today, especially if it's your first time. Our goal is simply to give you an idea what a data scientist may be doing and how you might use code to analyse data. \n",
    "\n",
    "We will point you in the direction of more educational content at the end of this workshop but it would be useful to quickly introduce a few things at this stage:\n",
    " - Variables – Today we will be working with objects called variables. We can name variables ourselves to reflect what they area. They might be numbers, a string of text or even bits of our data which we will see later.\n",
    " - Comments (#) – every time you see a '#' in a cell of code this means it iss a comment that will not effect the result. These are notes we can leave for ourselves to remind us what is happening.\n",
    "\n",
    "The cell below has an example of variables and comments. See if you can edit the code and see how the results change if you run the code again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df979516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell by pressing the run button or pressing Cntrl and Enter.\n",
    "# This cell has examples of variable assignment\n",
    "# (Notice how this line starts with '#' so it is a comment ignored by the code.)\n",
    "\n",
    "# vvv Try editing this code!\n",
    "my_name = \"Waldo\" \n",
    "breakfast_food = \"eggs\"\n",
    "fav_number = 17\n",
    "# ^^^ Try editing this code!\n",
    "\n",
    "print(\"Hello! My name is \" + my_name + \".\")\n",
    "print(\"Today I had \" + breakfast_food + \" for breakfast.\")\n",
    "print(str(fav_number) + \" is my favourite number.\")\n",
    "print(\"Twice my favourite number would be: \" + str(fav_number * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177542bf",
   "metadata": {},
   "source": [
    "# Getting Setup\n",
    "\n",
    "Next we'll start working with real data and get started with our analysis.\n",
    "\n",
    "Before we can do that we first need to load the packages and libraries we will use. These packages and libraries contain different tools we are going to use on our data. \n",
    "\n",
    "This is an important to step to make sure the rest of our code works – but don't worry too much about this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3503a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful Python modules\n",
    "\n",
    "print(\"About to import Python modules...\")\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "print(\"Finished importing Python modules\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb4f25",
   "metadata": {},
   "source": [
    "Next we’ll load in our data. The data we are using today is publicly available and contains over 400,000 news headlines from around the world that was collected in 2014.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will load in our dataset.\n",
    "\n",
    "# This is the name of the file, which is a csv file that has been compressed as a zip file.\n",
    "data_file_name = \"uci-news-aggregator-short.zip\"\n",
    "\n",
    "# Here we read file.\n",
    "news_df = pd.read_csv(data_file_name)\n",
    "\n",
    "# We're only going to look at titles and publishers.\n",
    "news_df = news_df.loc[:, ['TITLE','PUBLISHER']]\n",
    "\n",
    "#Let us know when the task is finished\n",
    "print(\"Finished reading the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f110bdb",
   "metadata": {},
   "source": [
    "The dataset is now stored in the variable named `news_df`. This variable contains an entire table of data!\n",
    "\n",
    "Lets see what our data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea7fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the table of data looks like:\n",
    "print(news_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717918f",
   "metadata": {},
   "source": [
    "The dataset is too large, so the notebook won't show us everything. It shows us the first 5 and last 5 nows in our data and tells us that we have 422,419 rows in total.\n",
    "\n",
    "Just from this quick look at the data, we see that this table has 3 columns:\n",
    "- a numbered label (also called the **index**) for each row\n",
    "- `TITLE`, containing the headline\n",
    "- `PUBLISHER`, who the headline was printed by\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7d8035",
   "metadata": {},
   "source": [
    "# Examining Publishers\n",
    "\n",
    "It would be useful to learn more about what is in our data. Let's look at how many different publishers there are in the `PUBLISHER` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16549f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the number of headlines per publisher in our dataset:\n",
    "\n",
    "publisher_counts = news_df['PUBLISHER'].value_counts()\n",
    "print(publisher_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030201ca",
   "metadata": {},
   "source": [
    "Again, this table is too large to easily print everything, but at a glance we can see that the publisher with the most headlines in this dataset is Reuters, followed by Huffington Post and Businessweek.\n",
    "\n",
    "`Length: 10985` also shows us there are 10,985 publishers in this dataset.\n",
    "\n",
    "We saved this table of publisher counts in the variable named `publisher_counts`, so we can use it in the next cell's code to see how many records appear with a given publisher. \n",
    "\n",
    "You can change the value of `publisher_to_check` to the name of any other news source you think might appear in this dataset. \n",
    "\n",
    "(Note: Python often makes use of indented code. If you edit the code we've written below, do not alter the indents at the start of lines. If you do, the code might not work correctly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8023624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at how many headlines in this dataset are from a given publisher.\n",
    "\n",
    "# vvv Try editing this code!\n",
    "publisher_to_check = \"Yorkshire Post\"\n",
    "# ^^^ Try editing this code!\n",
    "\n",
    "if publisher_to_check in publisher_counts:\n",
    "    print(publisher_counts[publisher_to_check])\n",
    "else:\n",
    "    print(\"That name doesn't appear in the list of publishers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2cf60e",
   "metadata": {},
   "source": [
    "# Examining Words in Headlines\n",
    "\n",
    "Now let's look at the headlines. Let's say we're interested in looking at the set of words in a headline. Here's a convenient method for splitting up a string of text into a list of words, separating it on all its spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f55577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting a string of text into separate words\n",
    "\n",
    "example_sentence = \"This example sentence contains six words.\"\n",
    "example_word_list = example_sentence.split()\n",
    "print(example_sentence)\n",
    "print(example_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d343a4d",
   "metadata": {},
   "source": [
    "We'll apply this to every headline in the `TITLE` column, and store all the results in a new column we'll call `WORDS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new WORDS column to our table\n",
    "\n",
    "news_df['WORDS'] = [headline.split() for headline in news_df['TITLE']]\n",
    "print(news_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167b4a7e",
   "metadata": {},
   "source": [
    "Here's some code that will show us how many headlines contain a given word, and shows the first few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f54d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of headlines with a chosen word\n",
    "\n",
    "# vvv Try editing this code!\n",
    "word_to_check = \"Sport\"\n",
    "# ^^^ Try editing this code!\n",
    "\n",
    "news_words_df = news_df.explode(\"WORDS\")\n",
    "word_appearances = news_words_df.loc[news_words_df['WORDS']==word_to_check]\n",
    "num_appearances = len(word_appearances)\n",
    "print(\"Number of appearances of \\\"\" + word_to_check + \"\\\": \" + str(num_appearances))\n",
    "print(\"Examples of appearances (maximum 10):\")\n",
    "print(word_appearances.iloc[:10,[0,1]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d8ccd",
   "metadata": {},
   "source": [
    "In the code above, you can change the value of `word_to_check` to be any word you'd like to search for. Try it yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2da84a",
   "metadata": {},
   "source": [
    "# Normalising Words\n",
    "\n",
    "After you've tried searching some words, go back and check how many times the word \"Sport\" appears. Now try checking \"sport\" in all lowercase instead -- you'll get a different output!\n",
    "\n",
    "Currently, the code would treat the same word with a capital letter as different, but that's not really what we want. When we're checking how frequently a word is used, we want it to count every version.\n",
    "\n",
    "Also, try checking \"Sport,\" with a comma after it -- that's treated differently too. So we should rewrite our code to ignore punctuation and capital letters. \n",
    "\n",
    "That's what the code in the next cell does, saving the normalised words by overwriting the `WORDS` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b91621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all punctuation and converting all words to lowercase\n",
    "\n",
    "nopunct_pattern = re.compile('[^a-zA-Z0-9 ]+')\n",
    "news_df['WORDS'] = [set(nopunct_pattern.sub('', headline.lower()).split()) for headline in news_df['TITLE']]\n",
    "print(news_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cbd2b0",
   "metadata": {},
   "source": [
    "Now when we search for words, the headlines with all possible capitalisations will be included. \n",
    "\n",
    "Try your searches again in this next cell of code. Again, feel free to change `word_to_check` to any word you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74061e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of headlines with a chosen word (now with normalised words)\n",
    "\n",
    "# vvv Try editing this code!\n",
    "word_to_check = \"Sport\"\n",
    "# ^^^ Try editing this code!\n",
    "\n",
    "word_to_check = word_to_check.lower() # Force lowercase\n",
    "news_words_df = news_df.explode(\"WORDS\")\n",
    "word_appearances = news_words_df.loc[news_words_df['WORDS']==word_to_check]\n",
    "num_appearances = len(word_appearances)\n",
    "print(\"Number of appearances of \\\"\" + word_to_check + \"\\\": \" + str(num_appearances))\n",
    "print(\"Examples of appearances (maximum 10):\")\n",
    "print(word_appearances.iloc[:10,[0,1]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ea984",
   "metadata": {},
   "source": [
    "# Exploring word frequency\n",
    "\n",
    "Next, let's compare how frequently certain words appear in headlines from different news sources. First let's take a look through the 30 publishers with the most headlines in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View 30 most common publishers in the dataset\n",
    "print(publisher_counts[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f10c169",
   "metadata": {},
   "source": [
    "From this list, we'll choose just a few to represent the UK and the US, in the code below.\n",
    "\n",
    "If you like, you could change these lists to your own choices of publishers, but make sure you write the publisher's names exactly as they appear in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b296eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare lists of UK and US publishers that we'd like to examine\n",
    "\n",
    "# vvv Try editing this code!\n",
    "uk_publisher_list = [\"Daily Mail\", \"Telegraph.co.uk\", \"The Guardian\"]\n",
    "us_publisher_list = [\"Los Angeles Times\", \"USA TODAY\", \"CBS Local\"]\n",
    "# ^^^ Try editing this code!\n",
    "\n",
    "print(publisher_counts[uk_publisher_list])\n",
    "print(\"Total for these publishers:\")\n",
    "print(sum(publisher_counts[uk_publisher_list]))\n",
    "print()\n",
    "print(publisher_counts[us_publisher_list])\n",
    "print(\"Total for these publishers:\")\n",
    "print(sum(publisher_counts[us_publisher_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2200ea4",
   "metadata": {},
   "source": [
    "Let's compare how many times the word \"the\" appears in these sources. You can also try replacing \"the\" with any other word you'd like to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting instances of a certain word in headlines from selected publishers\n",
    "\n",
    "# vvv Try editing this code!\n",
    "word_to_check = \"the\"\n",
    "# ^^^ Try editing this code!\n",
    "\n",
    "word_to_check = word_to_check.lower() # Force lowercase\n",
    "news_words_df = news_df.explode(\"WORDS\")\n",
    "word_appearances = news_words_df.loc[news_words_df['WORDS']==word_to_check].drop(['WORDS'], axis=1)\n",
    "\n",
    "word_appearances_uk = word_appearances[word_appearances[\"PUBLISHER\"].isin(uk_publisher_list)]\n",
    "word_appearances_us = word_appearances[word_appearances[\"PUBLISHER\"].isin(us_publisher_list)]\n",
    "\n",
    "total_uk_count = len(news_df[news_df[\"PUBLISHER\"].isin(uk_publisher_list)])\n",
    "total_us_count = len(news_df[news_df[\"PUBLISHER\"].isin(us_publisher_list)])\n",
    "\n",
    "print(\"Number of UK headlines with \\\"\" + word_to_check + \"\\\": \" + str(len(word_appearances_uk)) )\n",
    "print(\"Total UK headlines: \" + str(total_uk_count))\n",
    "print(\"Proportion: \" + str(len(word_appearances_uk)/total_uk_count))\n",
    "print()\n",
    "print(\"Number of US headlines with \\\"\" + word_to_check + \"\\\": \" + str(len(word_appearances_us)) )\n",
    "print(\"Total US headlines: \" + str(total_us_count))\n",
    "print(\"Proportion: \" + str(len(word_appearances_us)/total_us_count))\n",
    "print()\n",
    "print(word_appearances_uk)\n",
    "print(word_appearances_us)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048194e",
   "metadata": {},
   "source": [
    "Let's try to find which words appear in the most UK headlines and US headlines. You can change the value of the `top_words_number` to choose how many words you'd like to display, instead of only the top ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cc228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in headlines from our selected publishers\n",
    "\n",
    "# vvv Try editing this code!\n",
    "top_words_number = 10\n",
    "# ^^^ Try editing this code!\n",
    "\n",
    "uk_words_df = news_words_df[news_words_df[\"PUBLISHER\"].isin(uk_publisher_list)]\n",
    "uk_word_counts = uk_words_df['WORDS'].value_counts()\n",
    "print(str(top_words_number) + \" most common words from selected UK publishers:\")\n",
    "print(uk_word_counts[:top_words_number])\n",
    "print()\n",
    "us_words_df = news_words_df[news_words_df[\"PUBLISHER\"].isin(us_publisher_list)]\n",
    "us_word_counts = us_words_df['WORDS'].value_counts()\n",
    "print(str(top_words_number) + \" most common words from selected US publishers:\")\n",
    "print(us_word_counts[:top_words_number])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98ecc2",
   "metadata": {},
   "source": [
    "Many of these words aren't very interesting: to, the, in, of, and so on. \n",
    "\n",
    "Let's consult a list of these common words (also called \"stop words\"), and ignore anything on that list when counting the most frequent words in the headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff52b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Most common words in headlines from our selected publishers, ignoring stop words\n",
    "\n",
    "# vvv Try editing this code!\n",
    "top_words_number = 10\n",
    "# ^^^ Try editing this code!\n",
    "\n",
    "stopwords_list = [nopunct_pattern.sub('', sw.lower()) for sw in STOPWORDS]\n",
    "uk_goodwords_df = uk_words_df[~uk_words_df['WORDS'].isin(stopwords_list)]\n",
    "uk_goodword_counts = uk_goodwords_df['WORDS'].value_counts()\n",
    "us_goodwords_df = us_words_df[~us_words_df['WORDS'].isin(stopwords_list)]\n",
    "us_goodword_counts = us_goodwords_df['WORDS'].value_counts()\n",
    "\n",
    "print(str(top_words_number) + \" most common words from selected UK publishers:\")\n",
    "print(uk_goodword_counts[:top_words_number])\n",
    "print()\n",
    "print(str(top_words_number) + \" most common words from selected US publishers:\")\n",
    "print(us_goodword_counts[:top_words_number])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6026b7c0",
   "metadata": {},
   "source": [
    "# Bar Charts\n",
    "\n",
    "We can create bar charts showing some of the most common words in a group of publishers. Again, you can choose how many words we display in our chart by changing the value of the `top_words_number` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of UK data\n",
    "\n",
    "# This value is the number of most frequent words we'll include in our chart:\n",
    "# vvv Try editing this code!\n",
    "top_words_number = 10\n",
    "# ^^^ Try editing this code!\n",
    "\n",
    "word_counts_to_graph = uk_goodword_counts\n",
    "\n",
    "top_words_data = word_counts_to_graph[:top_words_number]\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(top_words_data.index, top_words_data.values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd1d28",
   "metadata": {},
   "source": [
    "The word labels run too closely together along the bottom of that chart, so let's try a horizontal bar chart instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar chart\n",
    "\n",
    "# This value is the number of most frequent words we'll include in our chart:\n",
    "# vvv Try editing this code!\n",
    "top_words_number = 10\n",
    "# ^^^ Try editing this code!\n",
    "\n",
    "word_counts_to_graph = uk_goodword_counts\n",
    "top_words_data = word_counts_to_graph[:top_words_number]\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(top_words_data.index[::-1], top_words_data.values[::-1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5f76a",
   "metadata": {},
   "source": [
    "There are several extra features that we can add to charts like this. Let's add a title for the chart, change the colour of the bars from the default blue, and add display the size of each bar as a label on the bar itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20249de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding extra features\n",
    "\n",
    "# vvv Try editing this code!\n",
    "\n",
    "# This value is the number of most frequent words we'll include in our chart:\n",
    "top_words_number = 10\n",
    "# This value is the colour of the bars:\n",
    "bar_colour = \"red\"\n",
    "# This value will be the title that appears at the top:\n",
    "chart_title = \"Most common words in UK headlines\"\n",
    "\n",
    "# ^^^ Try editing this code!\n",
    "\n",
    "\n",
    "word_counts_to_graph = uk_goodword_counts\n",
    "\n",
    "top_words_data = word_counts_to_graph[:top_words_number]\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.barh(top_words_data.index[::-1], top_words_data.values[::-1], color=bar_colour)\n",
    "ax.bar_label(bars, label_type='center')\n",
    "ax.set_title(chart_title)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce720c",
   "metadata": {},
   "source": [
    "# Word Clouds\n",
    "\n",
    "We can also make word clouds that show the most common words in our data.\n",
    "\n",
    "The algorithm for creating these word clouds has an element of randomness, so you can run the code multiple times to generate a slightly different-looking word cloud each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893cd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for UK headlines\n",
    "\n",
    "bg_colour = \"white\"\n",
    "\n",
    "text_for_cloud = \" \".join(uk_goodwords_df['WORDS'].tolist())\n",
    "wcloud = WordCloud(background_color=bg_colour, normalize_plurals=False).generate(text_for_cloud)\n",
    "plt.figure()\n",
    "plt.imshow(wcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b848125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for US headlines\n",
    "\n",
    "bg_colour = \"white\"\n",
    "\n",
    "text_for_cloud = \" \".join(us_goodwords_df['WORDS'].tolist())\n",
    "wcloud = WordCloud(background_color=bg_colour, normalize_plurals=False).generate(text_for_cloud)\n",
    "plt.figure()\n",
    "plt.imshow(wcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d28f3",
   "metadata": {},
   "source": [
    "# Exploring Word Differences\n",
    "\n",
    "The word clouds we just generated give us the impression that UK and US news publishers generally use different words in their headlines. This may be due to differences in topics covered, or differences in language usage.\n",
    "\n",
    "Let's try to find the words that are used more by one set of publishers than the other. There are many options for how we could measure this. Let's use the simple method of counting the number of times one set of publishers uses a word, minus the number of times the other set uses it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding words used more by one set of publishers vs the other\n",
    "\n",
    "# Getting list of words that appear in these sets' headlines, without stop words\n",
    "full_word_list = sorted(set(uk_goodword_counts.index) | set(us_goodword_counts.index))\n",
    "\n",
    "# Creating data frame that compares word counts\n",
    "full_uk_counts = [uk_goodword_counts[w] if w in uk_goodword_counts else 0 for w in full_word_list]\n",
    "full_us_counts = [us_goodword_counts[w] if w in us_goodword_counts else 0 for w in full_word_list]\n",
    "uk_vs_us_counts = pd.DataFrame({\"WORD\":full_word_list, \n",
    "                    \"UK_COUNT\":full_uk_counts, \n",
    "                    \"US_COUNT\":full_us_counts, \n",
    "                    \"UK_VS_US_DIFF\":[a-b for a,b in zip(full_uk_counts, full_us_counts)]})\n",
    "# Sorting data frame by difference in count\n",
    "uk_vs_us_counts.sort_values(by=\"UK_VS_US_DIFF\", ascending=False, inplace=True)\n",
    "print(uk_vs_us_counts[:10])\n",
    "print()\n",
    "print(uk_vs_us_counts[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f2809a",
   "metadata": {},
   "source": [
    "The above code lets us view the top 10 words that appear more from one publisher set than the other. Let's examine these tables to see what conclusions we can draw about this data.\n",
    "\n",
    "We see that the UK headlines mention \"UK\" more, and the US headlines mention \"California\" more, which is in line with how we'd expect geographic locations to be mentioned more often within their own areas. It may be surprising the \"US\" is mentioned more often by the UK data set, but the numbers are high for both sets. Also, the UK data set we chose is slightly larger than the US data set, so a more rigorous analysis would need to take that into account.\n",
    "\n",
    "We also notice certain topics arising more often in these different sets. The UK set mentions \"Kardashian\", \"Kim\", and \"Kanye\" more often, while the US set mentions \"GM\" (the company General Motors), \"stocks\", and \"tech\" more often. This is likely caused by our choice of publishers -- the Daily Mail provides more entertainment headlines about celebrities, while USA Today has a business section that regularly provides information about stocks. If we wanted to explore this further, we could try including different publishers, like the Financial Times.\n",
    "\n",
    "What if we want to study the usage of more regional language terms? Let's try searching for a few specific terms in the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae19148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vvv Try editing this code!\n",
    "list_of_words_to_check = [\"bap\", \"cob\", \"teacake\", \"barm\", \"knackered\", \"chuffed\", \"potluck\", \"faucet\"]\n",
    "# ^^^ Try editing this code!\n",
    "\n",
    "for word_to_check in list_of_words_to_check:\n",
    "    word_to_check = word_to_check.lower() # Force lowercase\n",
    "    news_words_df = news_df.explode(\"WORDS\")\n",
    "    word_appearances = news_words_df.loc[news_words_df['WORDS']==word_to_check].drop(['WORDS'], axis=1)\n",
    "    print(word_to_check + \" results: \" + str(word_appearances.shape[0]))\n",
    "    if word_appearances.shape[0] > 0:\n",
    "        print(word_appearances)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb93ce84",
   "metadata": {},
   "source": [
    "Nearly all our regional choices of words have turned up no results in this data set. Even if we broaden our regions to compare words only used in British English (knackered, chuffed) or American English (potluck, faucet), we still get nearly no results at all. We can conclude that in order to perform a more focused analysis of regional language variations, we would need a much larger data set that includes more variation in subject matter. It would also probably help to consult a data set with less formal language usage than news headlines, such as Twitter posts or chat forum messages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
